{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af0fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a60c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import layers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc33f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1820867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'modified_data/'\n",
    "like_samples = []\n",
    "dislike_samples = []\n",
    "labels = []\n",
    "for filename in os.scandir(directory):\n",
    "    data = pd.read_csv(filename.path)\n",
    "    if (data[\"label\"][0] == 'Like'):\n",
    "        labels.append(0)\n",
    "        like_samples.append(data.drop(['label'] , axis = 1))\n",
    "    else:\n",
    "        labels.append(1)\n",
    "        dislike_samples.append(data.drop(['label'] , axis = 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e285364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb79355",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_samples = np.array(like_samples)\n",
    "dislike_samples = np.array(dislike_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1401a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(461, 512, 14) (584, 512, 14)\n"
     ]
    }
   ],
   "source": [
    "print(like_samples.shape , dislike_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a366fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_like_size = int(len(like_samples) * 0.7)\n",
    "# test_like_size = len(like_labels) - train_like_size\n",
    "\n",
    "train_dislike_size = int(len(dislike_samples) * 0.7)\n",
    "# test_dislike_size =len(dislike_labels)  - train_dislike_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50685e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "print(train_like_size )\n",
    "print(train_dislike_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9236a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use stratified sampling to randomly select samples for the train and test sets\n",
    "X_train_like, X_test_like, y_train_like, y_test_like = train_test_split(\n",
    "   like_samples, [i for i in labels if i == 0],\n",
    "    train_size=train_like_size )\n",
    " \n",
    "X_train_dislike, X_test_dislike, y_train_dislike, y_test_dislike = train_test_split(\n",
    "    dislike_samples, [i for i in labels if i == 1] , \n",
    "    train_size = train_dislike_size)\n",
    "\n",
    "# Combine the selected samples to form the train and test sets\n",
    "X_train = np.concatenate((X_train_like, X_train_dislike))\n",
    "X_test = np.concatenate((X_test_like, X_test_dislike))\n",
    "y_train = np.concatenate((y_train_like, y_train_dislike))\n",
    "y_test = np.concatenate((y_test_like, y_test_dislike))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c4b1e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.concatenate((like_samples[41:], dislike_samples[42:]))\n",
    "# X_test = np.concatenate((like_samples[:41], dislike_samples[:42]))\n",
    "# y_train = np.concatenate(([0]*420, [1] * 542))\n",
    "# y_test = np.concatenate(([0]*41, [1] * 42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12a01d",
   "metadata": {},
   "source": [
    "# save train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d630bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('training-testing-data/testing/test_data', X_test, allow_pickle=True, fix_imports=True)\n",
    "# np.save('training-testing-data/testing/test_label', y_test, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "# np.save('training-testing-data/training/train_data', X_train, allow_pickle=True, fix_imports=True)\n",
    "# np.save('training-testing-data/training/train_label', y_train, allow_pickle=True, fix_imports=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e481515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input shape for CNN\n",
    "\n",
    "input_shape = (512,14,1)\n",
    "\n",
    "# Define CNN model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu' , input_shape = input_shape))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(164, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Dense(164, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(164, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.add(Dense(uni(ts=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "701ef020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3561eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/61 [==============================] - 5s 62ms/step - loss: 1.2419 - accuracy: 0.5561\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.6373 - accuracy: 0.6933\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.4488 - accuracy: 0.7661\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.3962 - accuracy: 0.8139\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.2941 - accuracy: 0.8732\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.2490 - accuracy: 0.8971\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.1933 - accuracy: 0.9220\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.1495 - accuracy: 0.9449\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.1351 - accuracy: 0.9470\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.1192 - accuracy: 0.9595\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.1045 - accuracy: 0.9647\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0932 - accuracy: 0.9699\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0706 - accuracy: 0.9813\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0609 - accuracy: 0.9813\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0530 - accuracy: 0.9875\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0523 - accuracy: 0.9813\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0505 - accuracy: 0.9834\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0452 - accuracy: 0.9875\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0426 - accuracy: 0.9906\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0364 - accuracy: 0.9896\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0274 - accuracy: 0.9938\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0204 - accuracy: 0.9948\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0318 - accuracy: 0.9938\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0258 - accuracy: 0.9958\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0183 - accuracy: 0.9969\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0156 - accuracy: 0.9969\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0210 - accuracy: 0.9948\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0356 - accuracy: 0.9875\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0262 - accuracy: 0.9917\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0160 - accuracy: 0.9938\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 4s 71ms/step - loss: 0.0246 - accuracy: 0.9917\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.0158 - accuracy: 0.9969\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.0152 - accuracy: 0.9969\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 4s 71ms/step - loss: 0.0120 - accuracy: 0.9990\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0098 - accuracy: 0.9990\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0118 - accuracy: 0.9958\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0136 - accuracy: 0.9969\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0075 - accuracy: 0.9990\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0180 - accuracy: 0.9948\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0144 - accuracy: 0.9948\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0220 - accuracy: 0.9917\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0359 - accuracy: 0.9896\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0220 - accuracy: 0.9896\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0279 - accuracy: 0.9906\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.0171 - accuracy: 0.9948\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0123 - accuracy: 0.9948\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.0234 - accuracy: 0.9896\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0075 - accuracy: 0.9990\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 4s 69ms/step - loss: 0.0129 - accuracy: 0.9958\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0030 - accuracy: 0.9990\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0125 - accuracy: 0.9938\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0291 - accuracy: 0.9917\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 4s 69ms/step - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 5s 74ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 5s 76ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.0042 - accuracy: 0.9979\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0090 - accuracy: 0.9969\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0054 - accuracy: 0.9979\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0019 - accuracy: 0.9990\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0041 - accuracy: 0.9979\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0058 - accuracy: 0.9969\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0065 - accuracy: 0.9979\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0050 - accuracy: 0.9990\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0099 - accuracy: 0.9958\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0047 - accuracy: 0.9990\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0145 - accuracy: 0.9938\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0045 - accuracy: 0.9979\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0118 - accuracy: 0.9958\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0192 - accuracy: 0.9979\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0042 - accuracy: 0.9979\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0033 - accuracy: 0.9990\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0062 - accuracy: 0.9969\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0033 - accuracy: 0.9990\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0101 - accuracy: 0.9979\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0049 - accuracy: 0.9979\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 8.8412e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0050 - accuracy: 0.9979\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 9.3282e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 2.9204e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 5.8568e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0056 - accuracy: 0.9979\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0035 - accuracy: 0.9979\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0132 - accuracy: 0.9948\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0092 - accuracy: 0.9969\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0055 - accuracy: 0.9979\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0030 - accuracy: 0.9979\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0044 - accuracy: 0.9979\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0145 - accuracy: 0.9958\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0094 - accuracy: 0.9948\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0075 - accuracy: 0.9979\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0043 - accuracy: 0.9969\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0074 - accuracy: 0.9979\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0232 - accuracy: 0.9948\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 8.0607e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0105 - accuracy: 0.9958\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.0026 - accuracy: 0.9990\n",
      "Epoch 141/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0078 - accuracy: 0.9979\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0101 - accuracy: 0.9979\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0095 - accuracy: 0.9969\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 8.3296e-04 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 7.9987e-04 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0131 - accuracy: 0.9969\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0062 - accuracy: 0.9969\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0049 - accuracy: 0.9979\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0102 - accuracy: 0.9979\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0041 - accuracy: 0.9979\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0022 - accuracy: 0.9990\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 4.5448e-04 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 9.8733e-04 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0026 - accuracy: 0.9990\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0021 - accuracy: 0.9990\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0030 - accuracy: 0.9990\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0029 - accuracy: 0.9979\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 9.8022e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0023 - accuracy: 0.9990\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 1.4748e-04 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0043 - accuracy: 0.9990\n",
      "Epoch 167/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0090 - accuracy: 0.9990\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0018 - accuracy: 0.9990\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 8.7314e-04 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0081 - accuracy: 0.9969\n",
      "Epoch 172/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 0.0066 - accuracy: 0.9990\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 4.9469e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 9.3431e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 3.0646e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 6.5980e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 8.2863e-04 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0055 - accuracy: 0.9990\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 7.1222e-04 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 2.9358e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0061 - accuracy: 0.9990\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 3.4961e-04 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0091 - accuracy: 0.9990\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 4.9752e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 2.3525e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 1.9113e-04 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 4s 61ms/step - loss: 3.2481e-04 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 2.6152e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 4s 59ms/step - loss: 0.0037 - accuracy: 0.9979\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.0053 - accuracy: 0.9990\n",
      "Epoch 193/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0114 - accuracy: 0.9958\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0028 - accuracy: 0.9990\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 8.2645e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.0039 - accuracy: 0.9979\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.0044 - accuracy: 0.9979\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0030 - accuracy: 0.9990\n",
      "Epoch 200/200\n",
      "42/61 [===================>..........] - ETA: 1s - loss: 3.0872e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "model.fit(X_train , y_train , epochs = 200 , batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dadf2609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "\n",
    "for data in X_test:\n",
    "    y_pred.append(np.argmax(model.predict(np.array([data , ]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e9a12",
   "metadata": {},
   "source": [
    "# evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "529aa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ee792d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    " \n",
    "#Plot the confusion matrix.\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=['Like','Dislike'],\n",
    "            yticklabels=['Like','Dislike'])\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8b69d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuaracy = (cm[0][0] + cm[1][1]) / X_test.shape[0] *100\n",
    "recall = (cm[0][0]) /(cm[0][0] + cm[1][0]) *100\n",
    "precision = (cm[0][0]) /(cm[0][0] + cm[0][1]) *100\n",
    "f1_score = (2*recall*precision)/(recall + precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8c186422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  85.07936507936508\n",
      "Precision:  88.48920863309353\n",
      "Recall:  79.87012987012987\n",
      "F1 SCore:  83.95904436860069\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ' , accuaracy)\n",
    "print('Precision: ' , precision)\n",
    "print('Recall: ' , recall)\n",
    "print('F1 SCore: ' , f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee3647",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c204219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/modified_data_single_fold\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/modified_data_single_fold\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/modified_data_single_fold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e566df1",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40261e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 510, 12, 32)       320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 510, 12, 32)      128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 255, 6, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 253, 4, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 253, 4, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 126, 2, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 126, 2, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8064)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 164)               1322660   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 164)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,332,814\n",
      "Trainable params: 1,332,686\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_model = tf.keras.models.load_model('models/modified_data_single_fold//')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd321bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 42ms/step - loss: 0.6143 - accuracy: 0.8508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6143319606781006, 0.8507936596870422]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate((np.concatenate((X_train,X_test))),np.concatenate((y_train,y_test)))\n",
    "new_model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
